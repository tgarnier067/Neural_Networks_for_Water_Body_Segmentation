{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e11d23f-3593-470c-bcab-0509ab44d839",
   "metadata": {},
   "source": [
    "Until now, we have used attention mechanisme into the residual connections. We want to try to use attention mechanisme, directly into the convolutions. We also include residual connection, into the convolutional block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cab2f7-b93f-4b23-9978-1b452af97012",
   "metadata": {},
   "source": [
    "We see that we do not have enough memory. We ask for 256 gib, while we only have 14.5. We tried to decrease the size of images to 128X128 instead of 256x256 => it uses 16gib, but there are two problems with that : it's hard to compare AA AER U-Net and AER U-Net, if they have different size of images, and 16 gib is still too much, and we do not want to decrease the size images. SO we decide to apply a window attention, instead of a full attention. The attention is based on a local part of the image, and not on the full image (see next notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513a841-d578-46dc-af02-cb015b14e6df",
   "metadata": {},
   "source": [
    "Since our AER U-Net architecture did not show consistent performance improvements over the standard U-Net, we investigated alternative ways of integrating attention and residual connections. In particular, we explored the use of Attention Augmented Convolutions (AAConv), as proposed by Bello et al. (ICCV 2019), which combine convolutional operations with multi-head self-attention to enhance the model’s capacity to capture long-range dependencies. However, implementing full spatial attention led to prohibitive memory usage, especially on high-resolution feature maps, which limited practical deployment on standard GPUs. As a compromise, we applied a more lightweight form of attention inside each convolutional block, inspired by Squeeze-and-Excitation (SE) modules (see next notebooks). This allowed us to retain some adaptive feature weighting at minimal computational cost, even if it does not fully replicate the spatial modeling capabilities of true AAConv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc4582-db10-45c0-ae5c-12c4867dffa2",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3229dffb-3a72-4b3b-97ac-401f089aca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision --quiet\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b38d83-cbb3-4868-9c46-549004335568",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (256, 256)\n",
    "\n",
    "def resize_image(image):\n",
    "    # image : numpy array ou PIL.Image\n",
    "    if isinstance(image, np.ndarray):\n",
    "        # Convert numpy array to PIL Image\n",
    "        image = Image.fromarray(image)\n",
    "    # Resize avec PIL\n",
    "    resized = image.resize(TARGET_SIZE, resample=Image.BILINEAR)\n",
    "    return np.array(resized)\n",
    "\n",
    "def mask_split_threshold(image):\n",
    "    # image numpy float ou uint8\n",
    "    image = image.astype(np.float32)\n",
    "    scaled = image / 255.0\n",
    "    return scaled\n",
    "\n",
    "def remove_padding(image):\n",
    "    # image numpy (H,W) ou (H,W,C)\n",
    "    if image.ndim == 3:\n",
    "        gray = np.array(Image.fromarray(image).convert('L'))\n",
    "    else:\n",
    "        gray = image\n",
    "    coords = np.argwhere(gray > 0)\n",
    "    if coords.size > 0:\n",
    "        y_min, x_min = coords.min(axis=0)\n",
    "        y_max, x_max = coords.max(axis=0)\n",
    "        cropped = image[y_min:y_max+1, x_min:x_max+1]\n",
    "        # Resize cropped à TARGET_SIZE\n",
    "        cropped_img = Image.fromarray(cropped)\n",
    "        cropped_resized = cropped_img.resize(TARGET_SIZE, resample=Image.BILINEAR)\n",
    "        return np.array(cropped_resized)\n",
    "    else:\n",
    "        img = Image.fromarray(image)\n",
    "        return np.array(img.resize(TARGET_SIZE, resample=Image.BILINEAR))\n",
    "\n",
    "def preprocess_image(pil_image):\n",
    "    image = np.array(pil_image)\n",
    "    image = resize_image(image)\n",
    "    image = remove_padding(image)\n",
    "    image = mask_split_threshold(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "class WaterBodyDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        mask = Image.open(self.mask_paths[idx]).convert('L')\n",
    "        \n",
    "        image = preprocess_image(image)\n",
    "        mask = preprocess_image(mask)\n",
    "        \n",
    "        image = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        mask = torch.tensor(mask).unsqueeze(0).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c8e707-f9a2-4231-b2bd-25ffb82440b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred_flat = pred.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    return (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35b2696-a6c2-496b-a357-d9802b468d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training\n",
    "\n",
    "# Get images\n",
    "images_dir = os.path.expanduser('~/work/Neural_Networks_for_Water_Body_Segmentation/Water_Bodies_Dataset/Images')\n",
    "masks_dir = os.path.expanduser('~/work/Neural_Networks_for_Water_Body_Segmentation/Water_Bodies_Dataset/Masks')\n",
    "\n",
    "image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))])\n",
    "mask_files = sorted([f for f in os.listdir(masks_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))])\n",
    "\n",
    "image_paths = [os.path.join(images_dir, f) for f in image_files]\n",
    "mask_paths = [os.path.join(masks_dir, f) for f in mask_files]\n",
    "\n",
    "\n",
    "# Build train and test sets\n",
    "train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n",
    "    image_paths, mask_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Generate data loader\n",
    "image_size = (256, 256)\n",
    "\n",
    "train_dataset = WaterBodyDataset(train_imgs, train_masks)\n",
    "val_dataset = WaterBodyDataset(val_imgs, val_masks)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94241641-38a0-4bdf-98d8-208487bff2d1",
   "metadata": {},
   "source": [
    "# Build the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd9e38a-9686-4d19-966a-f6eae367565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, dk=40, dv=4, Nh=4):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.dk = dk\n",
    "        self.dv = dv\n",
    "        self.Nh = Nh\n",
    "\n",
    "        self.query_conv = nn.Conv2d(out_channels, dk, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(out_channels, dk, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(out_channels, dv, kernel_size=1)\n",
    "        self.out_conv = nn.Conv2d(dv, out_channels, kernel_size=1)\n",
    "\n",
    "        self.scale = (dk // Nh) ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv(x)\n",
    "\n",
    "        batch_size, C, H, W = residual.size()\n",
    "        q = self.query_conv(residual).view(batch_size, self.Nh, self.dk // self.Nh, H*W)\n",
    "        k = self.key_conv(residual).view(batch_size, self.Nh, self.dk // self.Nh, H*W)\n",
    "        v = self.value_conv(residual).view(batch_size, self.Nh, self.dv // self.Nh, H*W)\n",
    "\n",
    "        q = q.permute(0,1,3,2)  # (B, Nh, HW, dk//Nh)\n",
    "        k = k.permute(0,1,2,3)  # (B, Nh, dk//Nh, HW)\n",
    "\n",
    "        attention = torch.matmul(q, k) * self.scale\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "\n",
    "        v = v.permute(0,1,3,2)\n",
    "        out = torch.matmul(attention, v)\n",
    "        out = out.permute(0,1,3,2).contiguous().view(batch_size, -1, H, W)\n",
    "        out = self.out_conv(out)\n",
    "\n",
    "        return residual + out\n",
    "\n",
    "\n",
    "class AAConvBlock(nn.Module):\n",
    "    \"\"\"AA Convolution block: AAConv2d + BatchNorm + ReLU\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            AAConv2d(in_channels, out_channels, kernel_size, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualAABlock(nn.Module):\n",
    "    \"\"\"Residual block with 2 AA conv blocks + dropout + skip connection\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = AAConvBlock(in_channels, out_channels)\n",
    "        self.conv2 = AAConvBlock(out_channels, out_channels)\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.skip_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip_conv(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = out + identity\n",
    "        return F.relu(out, inplace=False)\n",
    "\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Attention block for gating skip connections\"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi.clone()\n",
    "\n",
    "\n",
    "class AA_AER_UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder filter sizes\n",
    "        self.filters = [32, 64, 128]\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder1 = ResidualAABlock(in_channels, self.filters[0])\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.encoder2 = ResidualAABlock(self.filters[0], self.filters[1])\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.encoder3 = ResidualAABlock(self.filters[1], self.filters[2])\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualAABlock(self.filters[2], 256, dropout=0.3)\n",
    "\n",
    "        # Decoder\n",
    "        self.up3 = nn.ConvTranspose2d(256, self.filters[2], kernel_size=2, stride=2)\n",
    "        self.att3 = AttentionBlock(F_g=self.filters[2], F_l=self.filters[2], F_int=self.filters[2]//2)\n",
    "        self.decoder3 = ResidualAABlock(self.filters[2]*2, self.filters[2])\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(self.filters[2], self.filters[1], kernel_size=2, stride=2)\n",
    "        self.att2 = AttentionBlock(F_g=self.filters[1], F_l=self.filters[1], F_int=self.filters[1]//2)\n",
    "        self.decoder2 = ResidualAABlock(self.filters[1]*2, self.filters[1])\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(self.filters[1], self.filters[0], kernel_size=2, stride=2)\n",
    "        self.att1 = AttentionBlock(F_g=self.filters[0], F_l=self.filters[0], F_int=self.filters[0]//2)\n",
    "        self.decoder1 = ResidualAABlock(self.filters[0]*2, self.filters[0])\n",
    "\n",
    "        # Output\n",
    "        self.final_conv = nn.Conv2d(self.filters[0], out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.encoder1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "\n",
    "        e2 = self.encoder2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "\n",
    "        e3 = self.encoder3(p2)\n",
    "        p3 = self.pool3(e3)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p3)\n",
    "\n",
    "        # Decoder level 3\n",
    "        d3 = self.up3(b)\n",
    "        e3_att = self.att3(g=d3, x=e3)\n",
    "        d3 = torch.cat((e3_att, d3), dim=1)\n",
    "        d3 = self.decoder3(d3)\n",
    "\n",
    "        # Decoder level 2\n",
    "        d2 = self.up2(d3)\n",
    "        e2_att = self.att2(g=d2, x=e2)\n",
    "        d2 = torch.cat((e2_att, d2), dim=1)\n",
    "        d2 = self.decoder2(d2)\n",
    "\n",
    "        # Decoder level 1\n",
    "        d1 = self.up1(d2)\n",
    "        e1_att = self.att1(g=d1, x=e1)\n",
    "        d1 = torch.cat((e1_att, d1), dim=1)\n",
    "        d1 = self.decoder1(d1)\n",
    "\n",
    "        # Output\n",
    "        out = self.final_conv(d1)\n",
    "        return out # We apply the sigmoid function during the training, as we use the BCEWithLogitsLoss, it needs logits, and not predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaaf4b1-0b36-44b2-a21e-cdf127bd6e86",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724d7849-7464-4bd4-aa1e-67a438017a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   0%|          | 0/568 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.57 GiB of which 14.28 GiB is free. Process 117600 has 290.00 MiB memory in use. Of the allocated memory 160.33 MiB is allocated by PyTorch, and 9.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m masks = masks.to(device).float()\n\u001b[32m     20\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m loss = criterion(outputs, masks)\n\u001b[32m     23\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mAA_AER_UNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     e1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     p1 = \u001b[38;5;28mself\u001b[39m.pool1(e1)\n\u001b[32m    140\u001b[39m     e2 = \u001b[38;5;28mself\u001b[39m.encoder2(p1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mResidualAABlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     62\u001b[39m     identity = \u001b[38;5;28mself\u001b[39m.skip_conv(x)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(out)\n\u001b[32m     65\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.conv2(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mAAConvBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mAAConv2d.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     24\u001b[39m q = q.permute(\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# (B, Nh, HW, dk//Nh)\u001b[39;00m\n\u001b[32m     25\u001b[39m k = k.permute(\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m)  \u001b[38;5;66;03m# (B, Nh, dk//Nh, HW)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m attention = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.scale\n\u001b[32m     28\u001b[39m attention = F.softmax(attention, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     30\u001b[39m v = v.permute(\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m2\u001b[39m)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 256.00 GiB. GPU 0 has a total capacity of 14.57 GiB of which 14.28 GiB is free. Process 117600 has 290.00 MiB memory in use. Of the allocated memory 160.33 MiB is allocated by PyTorch, and 9.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model = AA_AER_UNet()\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for imgs, masks in loop:\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        train_preds.append(preds.cpu().numpy().flatten())\n",
    "        train_targets.append(masks.cpu().numpy().flatten())\n",
    "\n",
    "        preds_np = np.concatenate(train_preds)\n",
    "        targets_np = (np.concatenate(train_targets) > 0.5).astype(np.float32)\n",
    "        acc = accuracy_score(targets_np, preds_np)\n",
    "        loop.set_postfix(loss=np.mean(train_losses), accuracy=acc)\n",
    "\n",
    "    # evaluate the model after each epoch\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device).float()\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            val_loss = criterion(outputs, masks)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            val_preds.append(preds.cpu().numpy().flatten())\n",
    "            val_targets.append(masks.cpu().numpy().flatten())\n",
    "\n",
    "    val_loss_avg = np.mean(val_losses)\n",
    "    val_preds_np = np.concatenate(val_preds)\n",
    "    val_targets_np = (np.concatenate(val_targets) > 0.5).astype(np.float32)\n",
    "    val_acc = accuracy_score(val_targets_np, val_preds_np)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {np.mean(train_losses):.4f} - Train Acc: {acc:.4f} - Val Loss: {val_loss_avg:.4f} - Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbdcffb-e883-4407-aa2b-115fa7bed875",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bfdfa3-e437-4996-91a1-e60da24a9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "model.eval()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "val_losses = []\n",
    "val_preds = []\n",
    "val_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, masks in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device).float()\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, masks)\n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        val_preds.append(preds.cpu().numpy().flatten())\n",
    "        val_targets.append(masks.cpu().numpy().flatten())\n",
    "\n",
    "val_loss_avg = np.mean(val_losses)\n",
    "val_preds_np = np.concatenate(val_preds)\n",
    "val_targets_np = (np.concatenate(val_targets) > 0.5).astype(np.float32)\n",
    "\n",
    "val_acc = accuracy_score(val_targets_np, val_preds_np)\n",
    "val_precision = precision_score(val_targets_np, val_preds_np)\n",
    "val_recall = recall_score(val_targets_np, val_preds_np)\n",
    "val_f1 = f1_score(val_targets_np, val_preds_np)\n",
    "val_iou = jaccard_score(val_targets_np, val_preds_np)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss_avg:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "print(f\"Validation IoU Score: {val_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991927a2-1c27-499f-ba26-5bb45833b9ac",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7d812-e6bd-4421-b034-14ed3f3d5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "num_images_to_show = 4\n",
    "images_shown = 0\n",
    "rows = num_images_to_show\n",
    "cols = 3\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(12, 4 * rows))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, masks in val_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device).float()\n",
    "        outputs = model(imgs)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).float()\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        for i in range(batch_size):\n",
    "            if images_shown >= num_images_to_show:\n",
    "                break\n",
    "\n",
    "            axs[images_shown][0].imshow(imgs[i].cpu().permute(1, 2, 0))\n",
    "            axs[images_shown][0].set_title(\"Image\")\n",
    "            axs[images_shown][0].axis('off')\n",
    "\n",
    "            axs[images_shown][1].imshow(masks[i].cpu().squeeze(), cmap='gray')\n",
    "            axs[images_shown][1].set_title(\"Real Mask\")\n",
    "            axs[images_shown][1].axis('off')\n",
    "\n",
    "            axs[images_shown][2].imshow(preds[i].cpu().squeeze(), cmap='gray')\n",
    "            axs[images_shown][2].set_title(\"Prediction\")\n",
    "            axs[images_shown][2].axis('off')\n",
    "\n",
    "            images_shown += 1\n",
    "\n",
    "        if images_shown >= num_images_to_show:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
