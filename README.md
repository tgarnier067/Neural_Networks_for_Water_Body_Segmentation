# ğŸ§¹ OCR Text Cleaning with Small Language Models

This project aims to **automatically clean noisy OCR-generated text** using compact language models (LLMs). We use the OCR version of *The Vampyre* by John William Polidori as our case study.

We evaluate three individual LLMs and one hybrid approach for text correction, using two complementary evaluation methods:
- **LLM-as-a-Judge** (with **Gemini** as the evaluator),
- **Manual annotations** (performed by ourselves).

---

## ğŸ¯ Objectives

1. **Assess the reliability** of using LLMs as evaluators ("LLM-as-a-Judge") for OCR text correction.
2. **Explore the benefit of combining models** to improve text restoration performance.

---

## ğŸ™ GitHub

For a full overview, including detailed analysis of all tested models, please visit our GitHub repository, where the entire methodology is explained : https://github.com/tgarnier067/MNLP-project-2

## ğŸ“ Project Structure

### Paper

`MNLP_Homework_2_Pythoms.pdf` : **The paper we wrote, associated to this work**

---

### ğŸ“š Models Used

- `ocr`: Raw OCR output without any correction.
- `T5` : Correction output from the T5 model
- `bart`: Correction output from the BART model.
- `back_translation`: Classic back translation method.
- `back_translation_t5`: Back translation and T5 model.
- `gemini`: LLM model used for automatic evaluation (e.g., Gemini).

---

### ğŸ—‚ Notebooks

- `00_Data_extraction.ipynb`  
  Unzips the provided data archive containing OCR text.

- `01_Collect_data.ipynb`  
  Explores and documents the structure of the raw OCR dataset.

- `02_Models.ipynb`  
  Applies three different LLMs (T5, MarianMT, BART) and a combined approach (MarianMT â†’ T5) to correct the OCR text.  
  **Input**: Raw OCR text  
  **Output**: Cleaned text in CSV format: `{model}_correction.csv`

- `03_LLM_as_a_judge_{model}.ipynb`  
  Evaluates each corrected text using Gemini (LLM-as-a-Judge), which assigns scores (1â€“5) across text segments.  
  **Output**: `{model}_gemini_scores_all_texts.csv`

- `04_Students_as_a_judge_{model}.ipynb`  
  Manual annotation of the same text segments, based on the same evaluation criteria.  
  **Output**: `{model}_manual_scores.csv`

- `05_Correlation.ipynb`  
  Analyzes the agreement between Gemini and human scores to answer our research questions.

- `06_Output_files.ipynb`  
  Prepares final data outputs for submission: corrected texts, Gemini and manual annotations, and a 5,000-token extract from the best-performing model.

- `final_notebook.ipynb`  
  **A full project summary and walkthrough â€” designed for instructors or reviewers.**

---

### ğŸ“„ File Descriptions

- `{model}_correction`: Output generated by the `{model}` on the OCR input.
- `{model}_manual_scores`: Human annotation (manual evaluation) of the corrected output by `{model}`.
- `{model}_gemini_scores[_all_texts]`: Automatic evaluation of the `{model}` output using Gemini.

---

### ğŸ“Š Visualization Files

- `{model}_density.png`: Score distribution plots (manual or automatic) for `{model}`.
- `gemini_and_manual_density.png`: Comparison of Gemini vs. manual score distributions.
- `gemini_vs_manual_density.png`: Comparison of Gemini vs. manual score distributions.

---

### ğŸ§ª JSON Structured Files

- `Pythoms-hw2_ocr.json`: Raw OCR text data.
- `Pythoms-hw2_ocr_first_5k_tokens.json`: Subset containing the first 5,000 tokens of BART data, our best model.
- `Pythoms-hw2_ocr-{model}.json`: Structured output from `{model}`.
- `Pythoms-hw2_ocr-judge_{model}.json`: LLM as a judge judgments on the output of `{model}`.
- `Pythoms-hw2_ocr-manual_{model}.json`: Human judgments on the output of `{model}`

---

## ğŸ“Š Results & Insights

### ğŸ” Whatâ€™s the Best Model? Can LLM Combinations Boost Performance?

After extensive experimentation, we observed the following:

- **Translation models** like *MarianMT* are **ineffective for OCR correction** â€” they tend to distort the intended meaning rather than correct typographical errors.
- **T5** provides **moderate gains**, but not enough to outperform simpler baselines.
- **BART** consistently delivers the **best results across all metrics**.

To objectively assess the output quality, we used **ROUGE** scores â€” a standard metric for measuring overlap with ground-truth text.

| **Method**               | **ROUGE-1** | **ROUGE-2** | **ROUGE-L** |
|--------------------------|-------------|-------------|-------------|
| Back-Translation         | 0.7214      | 0.4295      | 0.6409      |
| **BART**                 | **0.8547**  | **0.7738**  | **0.8430**  |
| T5                       | 0.8296      | 0.7192      | 0.8185      |
| OCR (baseline)           | 0.8026      | 0.6577      | 0.8013      |
| Back-Translation + T5    | 0.7226      | 0.4541      | 0.6388      |

ğŸ“Œ **Takeaway**:  
**BART is the most effective model** for OCR post-correction in this setup. Interestingly, **model combinations did not outperform** standalone models.

---

### ğŸ§  LLM-as-a-Judge: Can We Trust Models Like Gemini to Evaluate Text?

We explored whether an LLM (Gemini) can reliably score and rank model outputs.

- **Gemini tends to overrate the raw OCR output**, often assigning it higher quality than warranted.
- Its scoring is **less nuanced**, avoiding extreme ratings.
- It **correctly identified the best (BART)** and **worst (Back-Translation)** performing models.
- However, it **misjudged T5**, ranking it below raw OCR â€” contradicting both human evaluation and ROUGE scores.

ğŸ“Œ **Takeaway**:  
While Gemini can spot clear differences in quality, it **lacks the sensitivity needed for finer distinctions**. Itâ€™s useful for quick assessments but **shouldnâ€™t be relied on for detailed model evaluation**.

---

## ğŸ“¬ Contact

If you have questions or would like to collaborate, feel free to reach out:

- **Thomas Garnier** â€“ tgarnier067@gmail.com  
- **Thomas Chen** â€“ thomaschen178@gmail.com